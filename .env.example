# Codex Configuration
# Copy this file to .env and fill in your values

# OpenAI API Key (Required)
OPENAI_API_KEY=placeholder

# Model Configuration
# Latest models (recommended)

# Short names (recommended format)
CODEX_MODEL=o4-mini
# CODEX_MODEL=o4

# Full names (alternative format)
# CODEX_MODEL=gpt-4o-mini
# CODEX_MODEL=gpt-4o

# Legacy models
# CODEX_MODEL=gpt-4
# CODEX_MODEL=gpt-4-turbo
# CODEX_MODEL=gpt-3.5-turbo

# Approval Mode (suggest, auto-edit, full-auto)
CODEX_APPROVAL_MODE=suggest

# Docker Configuration
DOCKER_WORKSPACE_PATH=/workspace

# Ollama Configuration (Local LLM)
# Set to true to use Ollama instead of OpenAI
USE_OLLAMA=true

# Ollama API URL (use localhost, never use IP address)
OLLAMA_API_URL=http://localhost:11434
# Ollama model to use (available models listed below)
OLLAMA_MODEL=codellama
# Higher token limit for Ollama models (32K context)
OLLAMA_MAX_TOKENS=32768
# Available Ollama models:
# OLLAMA_MODEL=codellama   # Best for coding tasks
# OLLAMA_MODEL=llama3      # Latest Llama model
# OLLAMA_MODEL=mistral     # Good general purpose model
# OLLAMA_MODEL=gemma       # Google's lightweight model
# OLLAMA_MODEL=phi         # Microsoft's efficient model
# OLLAMA_MODEL=mixtral     # Mixture of experts model

# Google Gemini Configuration
# GOOGLE_API_KEY=AIzaSyBCbdd8kRVEgxOeFCry2JytN3qyji3IlbU
# Available models (coding-focused models only):
# - gemini-pro-code (optimized for code generation and understanding)
# - gemini-1.5-pro-latest (latest high-performance model for coding)
# - gemini-1.5-flash-latest (faster model with good coding capabilities)

# Security Settings
# Set to true to disable network access in the sandbox
CODEX_DISABLE_NETWORK=false

# UI Settings
# Set to true to use dark mode
CODEX_DARK_MODE=false

# Advanced Settings
# Maximum tokens to generate
CODEX_MAX_COMPLETION_TOKENS=4096
# Temperature for generation (0.0-2.0)
CODEX_TEMPERATURE=0.7
